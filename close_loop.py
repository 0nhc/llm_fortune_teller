import re
import ast
import argparse
import os
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# get API keys from system environment variables
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
CHATGPT_API_KEY = os.getenv("CHATGPT_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

from gemini import GeminiInterface
from chatgpt import ChatGPTInterface
from deepseek import DeepSeekInterface
from claude import ClaudeInterface
from qwen import QwenInterface


# Initialize model interfaces
gemini = GeminiInterface(
    api_key=GEMINI_API_KEY,
    model_name="gemini-3-pro-preview",
    max_tokens=9600,
)

chatgpt = ChatGPTInterface(
    api_key=CHATGPT_API_KEY,
    model_name="gpt-5.2",
    max_tokens=9600,
)

deepseek = DeepSeekInterface(
    api_key=DEEPSEEK_API_KEY,
    model_name="deepseek-reasoner",
    max_tokens=9600,
)

# claude = ClaudeInterface(
#     api_key=CLAUDE_API_KEY,
#     model_name="claude-opus-4-5-20251101",
#     max_tokens=9600,
# )

# qwen = QwenInterface(
#     api_key=QWEN_API_KEY_SG,
#     model_name="qwen3-max-preview",
#     max_tokens=9600,
# )


def load_prompt(prefix: str = None) -> list:
    """
    Load prompt from file based on prefix.
    
    Args:
        prefix: Optional prefix for the prompt file (e.g., "wzw" -> "prompt_wzw.md")
        
    Returns:
        List containing the prompt string
    """
    if prefix:
        prompt_file = f"prompt_{prefix}.md"
    else:
        prompt_file = "prompt.md"
    
    with open(prompt_file, "r", encoding="utf-8") as f:
        prompt_str = f.read().strip()
    
    # Optional: wrap with Chinese language instruction
    prompt_str = "è¯·ç”¨ç®€ä½“ä¸­æ–‡å›ç­”ä¸‹é¢çš„é—®é¢˜ï¼š\n" + prompt_str
    return [prompt_str]


def parse_list_response(text: str):
    """
    1. Try strict ast.literal_eval (èƒ½è§£æå°±æœ€å¥½).
    2. If that fails, try stripping ``` fences and ast again.
    3. If still fails, cheap heuristic with 'true'/'false' in head.
    """
    # å…ˆè¯•ä¸€æ¬¡â€œå¹²å‡€â€çš„ ast
    try:
        data = ast.literal_eval(text)
        if (
            isinstance(data, list)
            and len(data) == 2
            and isinstance(data[0], bool)
            and isinstance(data[1], str)
        ):
            return data
    except Exception:
        pass

    # å¦‚æœ raw é‡Œé¢æœ‰ ``` ```ï¼Œå…ˆæŠŠ code fence å»æ‰ä¸€å±‚
    cleaned = text.strip()
    if "```" in cleaned:
        parts = cleaned.split("```")
        if len(parts) >= 3:
            cleaned = parts[1]
        cleaned = cleaned.strip()

    # å†è¯•ä¸€æ¬¡ ast.literal_eval
    try:
        data = ast.literal_eval(cleaned)
        if (
            isinstance(data, list)
            and len(data) == 2
            and isinstance(data[0], bool)
            and isinstance(data[1], str)
        ):
            return data
    except Exception:
        pass

    # æš´åŠ›æ–¹æ¡ˆï¼šçœ‹å‰ 200 ä¸ªå­—ç¬¦æœ‰æ²¡æœ‰ true/false
    head = cleaned[:200].lower()
    if "true" in head and "false" not in head:
        agree = True
    elif "false" in head and "true" not in head:
        agree = False
    else:
        agree = False

    # ç­”æ¡ˆç›´æ¥ç”¨åŸæ–‡ï¼ˆä½ åªæ˜¯è¦ä¼ ç»™ä¸‹ä¸€è½®å¼•ç”¨ï¼‰
    answer = text
    return [agree, answer]


def write_log_to_file(log_lines, filename: str = "dialog_log.md"):
    """
    Write dialog log to file.
    
    Args:
        log_lines: List of log entries to write
        filename: Output filename (can include directory path)
    """
    # Create directory if it doesn't exist
    dir_path = os.path.dirname(filename)
    if dir_path:
        os.makedirs(dir_path, exist_ok=True)
    with open(filename, "w", encoding="utf-8") as f:
        for entry in log_lines:
            f.write(entry.rstrip() + "\n\n")


def write_final_answers_to_file(
    gemini_answer: str,
    gpt_answer: str,
    deepseek_answer: str,
    # claude_answer: str,
    # qwen_answer: str,
    filename: str = "final_answers.md",
):
    """
    Write final answers to file.
    
    Args:
        gemini_answer: Gemini's final answer
        gpt_answer: ChatGPT's final answer
        deepseek_answer: DeepSeek's final answer
        filename: Output filename (can include directory path)
    """
    # Create directory if it doesn't exist
    dir_path = os.path.dirname(filename)
    if dir_path:
        os.makedirs(dir_path, exist_ok=True)
    with open(filename, "w", encoding="utf-8") as f:
        f.write("# Final Answers (Re-answered Original Prompt)\n")
        f.write(f"## Gemini final answer:\n{gemini_answer}\n\n")
        f.write(f"## ChatGPT final answer:\n{gpt_answer}\n\n")
        f.write(f"## DeepSeek final answer:\n{deepseek_answer}\n\n")
        # f.write(f"## Claude final answer:\n{claude_answer}\n\n")
        # f.write(f"## Qwen final answer:\n{qwen_answer}\n")


def close_loop_ask(
    prompt,
    max_loops=20,
    log_filename="dialog_log.md",
    final_answers_filename="final_answers.md",
    prefix=None,
):
    """
    Multi-model debate loop until all successfully returned models have agree=True.
    Models are not permanently offline; they are only marked as temporarily_down in the round where an error occurs.
    They will be retried in the next round.
    
    Args:
        prompt: List containing the prompt string
        max_loops: Maximum number of debate loops
        log_filename: Filename for the dialog log
        final_answers_filename: Filename for the final answers
        prefix: Optional prefix used for file naming (if None, uses default filenames)
    """

    def get_model(models, mid):
        for m in models:
            if m["id"] == mid:
                return m
        return None

    def build_debate_prompt(target_model, participants):
        name = target_model["name"]
        mid = target_model["id"]

        others = [m for m in participants if m["id"] != mid]
        if others:
            other_desc_lines = []
            for m in others:
                other_desc_lines.append(
                    f"ã€{m['name']} å½“å‰ç­”æ¡ˆã€‘\n{m['last_answer']}\n------------------------------\n"
                )
            others_block = "".join(other_desc_lines)
        else:
            others_block = "ï¼ˆå½“å‰åªæœ‰ä½ ä¸€ä¸ªæ¨¡å‹è¿”å›äº†ä¸Šä¸€è½®çš„ç­”æ¡ˆã€‚ï¼‰\n"

        # Header by model id /èƒ½åŠ›è¯´æ˜
        if mid == "gemini":
            header = (
                "ä½ æ˜¯ Google Gemini æ¨¡å‹ï¼Œå¹¶ä¸”å…·å¤‡è”ç½‘æ£€ç´¢ (web search) åŠŸèƒ½ã€‚\n\n"
                "ç°åœ¨æœ‰è‹¥å¹²ä¸ªæ¨¡å‹åœ¨å°±åŒä¸€ä¸ªç”¨æˆ·é—®é¢˜è¿›è¡Œå¤šè½®ä¸­æ–‡è¾©è®ºï¼Œä½ æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚\n"
                "æœ‰çš„æ¨¡å‹æ”¯æŒ web searchï¼Œæœ‰çš„æ¨¡å‹ä¸æ”¯æŒã€‚\n"
                "å¦‚æœä½ çœ‹åˆ°äº†å…¶ä»–æ¨¡å‹æå‡ºçš„äº’è”ç½‘æœç´¢è¯·æ±‚ï¼Œè¯·æ›¿å®ƒä»¬å®Œæˆæœç´¢ã€‚\n\n"
            )
        elif mid == "chatgpt":
            header = (
                "ä½ æ˜¯ OpenAI ChatGPT æ¨¡å‹ï¼Œå¹¶ä¸”å…·å¤‡è”ç½‘æ£€ç´¢ (web search) åŠŸèƒ½ã€‚\n\n"
                "ç°åœ¨æœ‰è‹¥å¹²ä¸ªæ¨¡å‹åœ¨å°±åŒä¸€ä¸ªç”¨æˆ·é—®é¢˜è¿›è¡Œå¤šè½®ä¸­æ–‡è¾©è®ºï¼Œä½ æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚\n"
                "æœ‰çš„æ¨¡å‹æ”¯æŒ web searchï¼Œæœ‰çš„æ¨¡å‹ä¸æ”¯æŒã€‚\n"
                "å¦‚æœä½ çœ‹åˆ°äº†å…¶ä»–æ¨¡å‹æå‡ºçš„äº’è”ç½‘æœç´¢è¯·æ±‚ï¼Œè¯·æ›¿å®ƒä»¬å®Œæˆæœç´¢ã€‚\n\n"
            )
        elif mid == "claude":
            header = (
                "ä½ æ˜¯ Anthropic Claude æ¨¡å‹ï¼Œå¹¶ä¸”å…·å¤‡è”ç½‘æ£€ç´¢ (web search) åŠŸèƒ½ã€‚\n\n"
                "ç°åœ¨æœ‰è‹¥å¹²ä¸ªæ¨¡å‹åœ¨å°±åŒä¸€ä¸ªç”¨æˆ·é—®é¢˜è¿›è¡Œå¤šè½®ä¸­æ–‡è¾©è®ºï¼Œä½ æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚\n"
                "æœ‰çš„æ¨¡å‹æ”¯æŒ web searchï¼Œæœ‰çš„æ¨¡å‹ä¸æ”¯æŒã€‚\n"
                "å¦‚æœä½ çœ‹åˆ°äº†å…¶ä»–æ¨¡å‹æå‡ºçš„äº’è”ç½‘æœç´¢è¯·æ±‚ï¼Œè¯·æ›¿å®ƒä»¬å®Œæˆæœç´¢ã€‚\n\n"
            )
        elif mid == "qwen":
            header = (
                "ä½ æ˜¯é˜¿é‡Œé€šä¹‰åƒé—®ï¼ˆQwenï¼‰å¤§æ¨¡å‹ï¼Œç›®å‰é€šè¿‡å…¼å®¹ OpenAI åè®®è¢«è°ƒç”¨ï¼Œ\n"
                "åœ¨æœ¬ç¯å¢ƒä¸‹ä¸æ”¯æŒä¸»åŠ¨è”ç½‘æ£€ç´¢ã€‚\n\n"
                "ç°åœ¨æœ‰è‹¥å¹²ä¸ªæ¨¡å‹åœ¨å°±åŒä¸€ä¸ªç”¨æˆ·é—®é¢˜è¿›è¡Œå¤šè½®ä¸­æ–‡è¾©è®ºï¼Œä½ æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚\n"
                "å…¶å®ƒéƒ¨åˆ†æ¨¡å‹å¯ä»¥é€šè¿‡ web search è·å–å¤–éƒ¨ä¿¡æ¯ï¼Œä½ å¯ä»¥å‚è€ƒå®ƒä»¬åœ¨ç­”æ¡ˆä¸­ç»™å‡ºçš„å¼•ç”¨å’Œé“¾æ¥ã€‚\n"
                "å¦‚æœä½ æœ‰ä¸ç¡®å®šçš„åœ°æ–¹éœ€è¦è®¿é—®äº’è”ç½‘æœç´¢ä¿¡æ¯ï¼Œä½ å¯ä»¥åœ¨è¾“å‡ºå›ç­”ä¸­è¯·æ±‚å…¶ä»–æ¨¡å‹æ›¿ä½ æœç´¢ã€‚\n\n"
            )
        else:  # deepseek
            header = (
                "ä½ æ˜¯ DeepSeek æ¨¡å‹ deepseek-reasonerï¼Œç›®å‰ä¸æ”¯æŒä¸»åŠ¨è”ç½‘æ£€ç´¢ã€‚\n\n"
                "ç°åœ¨æœ‰è‹¥å¹²ä¸ªæ¨¡å‹åœ¨å°±åŒä¸€ä¸ªç”¨æˆ·é—®é¢˜è¿›è¡Œå¤šè½®ä¸­æ–‡è¾©è®ºï¼Œä½ æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚\n"
                "å…¶å®ƒéƒ¨åˆ†æ¨¡å‹å¯ä»¥é€šè¿‡ web search è·å–å¤–éƒ¨ä¿¡æ¯ï¼Œä½ å¯ä»¥å‚è€ƒå®ƒä»¬åœ¨ç­”æ¡ˆä¸­ç»™å‡ºçš„å¼•ç”¨å’Œé“¾æ¥ã€‚\n"
                "å¦‚æœä½ æœ‰ä¸ç¡®å®šçš„åœ°æ–¹éœ€è¦è®¿é—®äº’è”ç½‘æœç´¢ä¿¡æ¯ï¼Œä½ å¯ä»¥åœ¨è¾“å‡ºå›ç­”ä¸­è¯·æ±‚å…¶ä»–æ¨¡å‹æ›¿ä½ æœç´¢ã€‚\n\n"
            )
        meta = (
            "æœ¬è½®ä½ å°†çœ‹åˆ°å…¶å®ƒæ¨¡å‹æœ€æ–°çš„è¾“å‡ºã€‚\n"
            "------------------------------\n"
            f"{others_block}\n"
            "è¯·ä½ åœ¨å……åˆ†ç†è§£è¿™äº›å†…å®¹çš„å‰æä¸‹ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n"
            "1. æ‰¹åˆ¤æ€§åœ°åˆ¤æ–­ä½ æ˜¯å¦åŒæ„å…¶å®ƒæ¨¡å‹å½“å‰ç­”æ¡ˆçš„å…¨éƒ¨ç»“è®ºä¸æ¨ç†è¿‡ç¨‹ã€‚\n"
            "2. å°è¯•å¯¹ä¸åŒè§‚ç‚¹è¿›è¡Œå½’ç±»ï¼šè°å’Œä½ æ›´æ¥è¿‘ï¼Ÿè°å‡ºç°äº†æ˜æ˜¾é”™è¯¯æˆ–é—æ¼ï¼Ÿ\n"
            "3. å¦‚æœä½ å‘ç°è‡ªå·±åŸå§‹åˆ¤æ–­æœ‰é”™è¯¯æˆ–ä¸å¤Ÿä¸¥è°¨ï¼Œè¯·å¦ç„¶æ‰¿è®¤å¹¶æ˜ç¡®å†™å‡ºä½ è¢«è¯´æœ/æ›´æ–°ç«‹åœºçš„åœ°æ–¹ã€‚\n"
            "4. åœ¨ç»™å‡ºæœ¬è½®æ–°çš„è§‚ç‚¹æ—¶ï¼Œè¯·å°½å¯èƒ½ï¼š\n"
            "   - æŒ‡å‡ºå“ªäº›ç»“è®ºå·²ç»è¾¾æˆå…±è¯†ï¼›\n"
            "   - å¯¹å°šæœ‰åˆ†æ­§çš„ç‚¹ï¼Œç»™å‡ºæ›´å¼ºçš„è®ºè¯ï¼Œæˆ–è€…æå‡ºæ–¹æ¡ˆä»¥å¸®åŠ©å…¶å®ƒæ¨¡å‹å‘ä½ é æ‹¢ï¼›\n"
            "   - é¿å…é‡å¤æ•´ç¯‡é‡å†™åŸç­”æ¡ˆï¼Œé‡ç‚¹å†™â€œåˆ†æ­§ç‚¹ + å…±è¯†çš„æ”¶æŸæ–¹å¼â€ã€‚\n"
            "5. è¯·ç›´æ¥è¾“å‡ºä½ æƒ³å¯¹å…¶ä»–æ¨¡å‹è¯´çš„è¯ï¼Œè¯•å›¾è¾©è®ºã€è¯´æœå¯¹æ–¹ã€‚\n"
            "6. æŠŠç¯‡å¹…æ›´å¤šç”¨åœ¨ï¼šä¿®æ­£ã€è¯´æœã€æ•´åˆåˆ†æ­§ä¸Šã€‚\n"
        )
        # meta = (
        #     "æœ¬è½®ä½ å°†çœ‹åˆ°å…¶å®ƒæ¨¡å‹å½“å‰æœ€æ–°çš„ç­”æ¡ˆç‰ˆæœ¬ã€‚ä½ çš„ç›®æ ‡ä¸æ˜¯æ­»å®ˆè‡ªå·±çš„åŸå§‹è§‚ç‚¹ï¼Œ\n"
        #     "è€Œæ˜¯ï¼šåœ¨ä¿è¯é€»è¾‘ä¸¥è°¨å’Œäº‹å®å¯é çš„å‰æä¸‹ï¼Œ**å°½å¯èƒ½ä¿ƒæˆæ‰€æœ‰æ¨¡å‹åœ¨å…³é”®ç»“è®ºä¸Šçš„æ”¶æ•›å’Œå…±è¯†**ã€‚\n"
        #     "å¦‚æœä½ è®¤ä¸ºæŸä¸ªæ¨¡å‹çš„è®ºè¯æ¯”ä½ åŸæ¥æ›´ä¸¥è°¨ã€æ›´æœ‰è¯´æœåŠ›ï¼Œå¯ä»¥æ˜ç¡®å†™å‡ºä½ åœ¨å“ªäº›ç‚¹ä¸Šè¢«è¯´æœã€æ„¿æ„ä¿®æ­£ç«‹åœºã€‚\n"
        #     "å¦‚æœä½ ä¸æŸä¸ªæ¨¡å‹è§‚ç‚¹ç›¸è¿‘ï¼Œè€Œæœ‰ç¬¬ä¸‰ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰æ¨¡å‹è§‚ç‚¹æ˜æ˜¾æœ‰é—®é¢˜ï¼Œä½ å¯ä»¥æœ‰æ„è¯†åœ°ä¸è§‚ç‚¹ç›¸è¿‘çš„ä¸€æ–¹å½¢æˆâ€œè”ç›Ÿâ€ï¼Œ\n"
        #     "ä¸€èµ·ç”¨æ›´ç³»ç»Ÿçš„è®ºè¯å»è¯´æœé‚£ä¸€æ–¹ä¿®æ”¹è§‚ç‚¹ã€‚\n\n"
        #     "ä¸‹é¢æ˜¯å…¶å®ƒæ¨¡å‹å½“å‰çš„ç­”æ¡ˆï¼ˆå®ƒä»¬éƒ½åœ¨å›ç­”åŒä¸€ä¸ªåŸå§‹é—®é¢˜ï¼‰ï¼š\n"
        #     "------------------------------\n"
        #     f"{others_block}\n"
        #     "è¯·ä½ åœ¨å……åˆ†ç†è§£è¿™äº›å†…å®¹çš„å‰æä¸‹ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n"
        #     "1. æ‰¹åˆ¤æ€§åœ°åˆ¤æ–­ä½ æ˜¯å¦ä¸¥æ ¼æ„ä¹‰ä¸Šå®Œå…¨åŒæ„å…¶å®ƒæ¨¡å‹å½“å‰ç­”æ¡ˆçš„å…¨éƒ¨å…³é”®ç»“è®ºä¸æ¨ç†è¿‡ç¨‹ã€‚\n"
        #     "2. å°è¯•å¯¹ä¸åŒè§‚ç‚¹è¿›è¡Œå½’ç±»ï¼šè°å’Œä½ æ›´æ¥è¿‘ï¼Ÿè°å‡ºç°äº†æ˜æ˜¾é”™è¯¯æˆ–é—æ¼ï¼Ÿ\n"
        #     "3. å¦‚æœä½ å‘ç°è‡ªå·±åŸå§‹åˆ¤æ–­æœ‰é”™è¯¯æˆ–ä¸å¤Ÿä¸¥è°¨ï¼Œè¯·å¦ç„¶æ‰¿è®¤å¹¶æ˜ç¡®å†™å‡ºä½ è¢«è¯´æœ/æ›´æ–°ç«‹åœºçš„åœ°æ–¹ã€‚\n"
        #     "4. åœ¨ç»™å‡ºæœ¬è½®æ–°çš„è§‚ç‚¹æ—¶ï¼Œè¯·å°½å¯èƒ½ï¼š\n"
        #     "   - æŒ‡å‡ºå“ªäº›ç»“è®ºå·²ç»è¾¾æˆå…±è¯†ï¼›\n"
        #     "   - å¯¹å°šæœ‰åˆ†æ­§çš„ç‚¹ï¼Œç»™å‡ºæ›´å¼ºçš„è®ºè¯ï¼Œæˆ–è€…æå‡ºæŠ˜ä¸­æ–¹æ¡ˆï¼Œä»¥å¸®åŠ©å…¶å®ƒæ¨¡å‹å‘ä½ é æ‹¢ï¼›\n"
        #     "   - é¿å…é‡å¤æ•´ç¯‡é‡å†™åŸç­”æ¡ˆï¼Œé‡ç‚¹å†™â€œåˆ†æ­§ç‚¹ + å…±è¯†çš„æ”¶æŸæ–¹å¼â€ã€‚\n"
        #     "5. è¾“å‡ºæ—¶ä»ç„¶éœ€è¦ç»™å‡ºä½ è®¤ä¸ºå½“å‰æœ€æ­£ç¡®ã€æœ€å®Œæ•´çš„ä¸­æ–‡ç­”æ¡ˆï¼Œä½†å¯ä»¥é€‚å½“ç®€åŒ–å¯¹åŸºç¡€èƒŒæ™¯çš„é‡å¤æè¿°ï¼Œ\n"
        #     "   æŠŠç¯‡å¹…æ›´å¤šç”¨åœ¨ï¼šä¿®æ­£ã€è¯´æœã€æ•´åˆåˆ†æ­§ä¸Šã€‚\n"
        # )

        if target_model["supports_web"]:
            web_part = (
                "\nâš ï¸ å…³äºè”ç½‘æ£€ç´¢ï¼š\n"
                "  - ä½ å…·å¤‡ web search èƒ½åŠ›ï¼Œè¯·åœ¨éœ€è¦äº‹å®ä¿¡æ¯æˆ–æœ€æ–°èµ„æ–™æ—¶ä¸»åŠ¨ä½¿ç”¨æœç´¢ã€‚\n"
                "  - å¦‚æœæœ¬è½®ä½¿ç”¨äº† web searchï¼Œè¯·åœ¨ç­”æ¡ˆæœ«å°¾åˆ—å‡ºâ€œå‚è€ƒé“¾æ¥ï¼šâ€å¹¶ç»™å‡ºå…³é”® URLï¼›\n"
                "    å¦‚æœæ²¡æœ‰ä½¿ç”¨ä»»ä½•å¤–éƒ¨ç½‘é¡µï¼Œå¯ä»¥å†™â€œå‚è€ƒé“¾æ¥ï¼šæ— â€ã€‚\n"
            )
        else:
            web_part = (
                "\nâš ï¸ å…³äºè”ç½‘æ£€ç´¢ï¼š\n"
                "  - ä½ ä¸èƒ½ä¸»åŠ¨è®¿é—®äº’è”ç½‘ï¼Œä½†å¯ä»¥ä¾èµ–ä½ è‡ªèº«çš„çŸ¥è¯†å’Œå…¶å®ƒæ¨¡å‹ç»™å‡ºçš„å‚è€ƒé“¾æ¥ã€‚\n"
                "  - å¦‚æœä½ åœ¨æœ¬è½®è®ºè¯ä¸­å‚è€ƒäº†å…¶å®ƒæ¨¡å‹ç»™å‡ºçš„é“¾æ¥ï¼Œå¯ä»¥åœ¨ç­”æ¡ˆæœ«å°¾åˆ—å‡ºâ€œå‚è€ƒé“¾æ¥ï¼šâ€å¹¶æ ‡æ³¨â€œè½¬å¼•è‡ªæŸæ¨¡å‹â€ã€‚\n"
            )

        tail = (
            "\nâš ï¸ è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š\n"
            "  - ä½ å¿…é¡»è¿”å›ä¸€ä¸ª Python åˆ—è¡¨ï¼Œé•¿åº¦ä¸º 2ï¼š\n"
            "    ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¸ƒå°”å€¼ True æˆ– Falseï¼Œè¡¨ç¤ºä½ æ˜¯å¦è®¤ä¸ºâ€œç›®å‰æ‰€æœ‰åœ¨çº¿å¹¶æˆåŠŸè¿”å›çš„æ¨¡å‹çš„ç­”æ¡ˆå·²ç»åœ¨å…³é”®ç»“è®ºä¸Šè¶³å¤Ÿä¸€è‡´ï¼Œå¯ä»¥è§†ä¸ºè¾¾æˆå…±è¯†â€ã€‚\n"
            "    ç¬¬äºŒä¸ªå…ƒç´ æ˜¯ä½ çš„ä¸­æ–‡ç­”æ¡ˆå­—ç¬¦ä¸²ï¼ˆå¿…é¡»æ˜¯ç®€ä½“ä¸­æ–‡ï¼Œä¸è¦ä½¿ç”¨è‹±æ–‡ï¼‰ã€‚\n"
        )

        return header + meta + web_part + tail

    def build_final_prompt(target_model):
        name = target_model["name"]
        supports_web = target_model["supports_web"]

        head = (
            f"ç°åœ¨ï¼Œä½ å’Œå…¶å®ƒåœ¨çº¿æ¨¡å‹å·²ç»å°±è¯¥é—®é¢˜è¾¾æˆäº†å®è´¨æ€§çš„å…±è¯†ã€‚\n"
            f"ä½ æ˜¯ {name}ï¼Œæ¥ä¸‹æ¥è¯·ä½ é¢å‘ç”¨æˆ·ï¼Œç»™å‡ºä¸€ä»½â€œæœ€ç»ˆé•¿ç¯‡è§£è¯»â€ã€‚\n"
            "ç”¨æˆ·ä¸å…³å¿ƒæ¨¡å‹ä¹‹é—´çš„è¾©è®ºè¿‡ç¨‹ï¼Œåªå…³å¿ƒæœ€åæ•´åˆåçš„ã€å°½å¯èƒ½å®Œæ•´è€Œæœ‰æ·±åº¦çš„ç­”æ¡ˆã€‚\n\n"
        )

        body = (
            "ã€å†™ä½œè¦æ±‚ã€‘\n"
            "1. ä½¿ç”¨**ç®€ä½“ä¸­æ–‡**ï¼Œé£æ ¼å¯ä»¥è‡ªç„¶ã€æœ‰ä¸€ç‚¹ä½ çš„â€œä¸ªæ€§â€ï¼Œä½†è¦ä¿è¯é€»è¾‘æ¸…æ™°ã€‚\n"
            "2. ä¸è¦å†æåŠâ€œæ¨¡å‹â€â€œè¾©è®ºâ€â€œè°è¯´è¿‡ä»€ä¹ˆâ€ç­‰è¿‡ç¨‹æ€§ä¿¡æ¯ï¼Œ\n"
            "   å°±åƒæ˜¯ä½ ç‹¬ç«‹æ€è€ƒåç»™å‡ºçš„ç»ˆææŠ¥å‘Šã€‚\n"
            "3. è¯·å°½é‡å†™å¾—**è¯¦ç»†ã€å……å®ã€æœ‰æƒ³è±¡åŠ›**ï¼š\n"
            "   - æ€»è§ˆ/ç»“è®ºç»¼è¿°ï¼šå…ˆç”¨ 1â€“3 æ®µè¯æ¦‚æ‹¬æ ¸å¿ƒç»“è®ºå’Œæ•´ä½“å°è±¡ï¼›\n"
            "   - åˆ†ç« èŠ‚å±•å¼€ï¼šæŒ‰è‹¥å¹²ç»´åº¦æ‹†è§£é—®é¢˜ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½è¦æœ‰æ¸…æ¥šçš„æ¨ç†å’Œä¾‹å­ï¼›\n"
            "   - é£é™©ä¸å±€é™ï¼šæŒ‡å‡ºå“ªäº›åœ°æ–¹å­˜åœ¨ä¸ç¡®å®šæ€§ã€å®¹æ˜“è¢«è¯¯è¯»ï¼›\n"
            "   - æ€»ç»“ä¸è¡ŒåŠ¨å»ºè®®ï¼šä»å®è§‚ä¸Šå†æ”¶æŸä¸€æ¬¡ï¼Œå¹¶ç»™å‡ºä¸‹ä¸€æ­¥å¯ä»¥å¦‚ä½•ç†è§£/è¡ŒåŠ¨ã€‚\n"
            "4. å­—æ•°ä¸Šä¸è¦åå•¬ï¼Œåªè¦ä¿¡æ¯æ˜¯æœ‰ç”¨çš„ã€æ¨ç†æ˜¯æœ‰ä»·å€¼çš„ï¼Œå¯ä»¥å†™å¾—å¾ˆé•¿ï¼ˆä¾‹å¦‚ 3000 å­—ä»¥ä¸Šï¼‰ã€‚\n"
            "5. å¦‚æœä½ åœ¨ä¹‹å‰è½®æ¬¡ä¸­ä½¿ç”¨è¿‡ web search æˆ–å‚è€ƒè¿‡é“¾æ¥ï¼Œè¯·åœ¨æ­£æ–‡ä¸­è‡ªç„¶å¸æ”¶è¿™äº›ä¿¡æ¯ï¼Œ\n"
            "   å¹¶åœ¨ç­”æ¡ˆæœ«å°¾åˆ—å‡ºâ€œå‚è€ƒé“¾æ¥ï¼šâ€éƒ¨åˆ†ï¼Œé€æ¡ç»™å‡ºä½ è®¤ä¸ºå…³é”®çš„ URLï¼›\n"
            "   å¦‚æœæ²¡æœ‰ä½¿ç”¨ä»»ä½•å¤–éƒ¨ç½‘é¡µï¼Œå¯ä»¥å†™â€œå‚è€ƒé“¾æ¥ï¼šæ— â€ã€‚\n\n"
            "è¯·ç›´æ¥è¾“å‡ºè¿™ä¸€ä»½é•¿ç¯‡æœ€ç»ˆç­”æ¡ˆçš„å®Œæ•´å†…å®¹ã€‚\n"
        )

        if not supports_web:
            extra = (
                "ä½ ä¸èƒ½ä¸»åŠ¨è®¿é—®äº’è”ç½‘ï¼Œä½†å¯ä»¥å¼•ç”¨ä½ åœ¨ä¹‹å‰è½®æ¬¡ä¸­ä»å…¶å®ƒæ¨¡å‹çœ‹åˆ°çš„é“¾æ¥æˆ–å¤–éƒ¨ä¿¡æ¯ï¼Œ\n"
                "å¦‚æœæœ‰å‚è€ƒè¿™äº›å†…å®¹ï¼Œè¯·åœ¨â€œå‚è€ƒé“¾æ¥ï¼šâ€éƒ¨åˆ†æ³¨æ˜â€œè½¬å¼•è‡ªå…¶å®ƒæ¨¡å‹ï¼šURLâ€ã€‚\n\n"
            )
            return head + extra + body
        else:
            return head + body

    # ================== æ­£å¼å¼€å§‹ ==================

    log = []
    prompt_text = " ".join(str(p) for p in prompt)
    log.append(
        "=== Initial User Prompt ===\n"
        f"{prompt_text}\n"
        f"(timestamp: {datetime.now().isoformat()})"
    )

    # åˆå§‹åŒ–æ¨¡å‹çŠ¶æ€ï¼ˆä¸å†æœ‰æ°¸ä¹… activeï¼Œä»…æœ‰ temporarily_downï¼‰
    models = [
        {
            "id": "gemini",
            "name": "Google Gemini",
            "interface": gemini,
            "supports_web": True,
            "temporarily_down": False,
            "last_answer": "",
            "last_struct": "",
            "last_agree": False,
        },
        {
            "id": "chatgpt",
            "name": "OpenAI ChatGPT",
            "interface": chatgpt,
            "supports_web": True,
            "temporarily_down": False,
            "last_answer": "",
            "last_struct": "",
            "last_agree": False,
        },
        {
            "id": "deepseek",
            "name": "DeepSeek",
            "interface": deepseek,
            "supports_web": False,
            "temporarily_down": False,
            "last_answer": "",
            "last_struct": "",
            "last_agree": False,
        },
        # {
        #     "id": "claude",
        #     "name": "Anthropic Claude",
        #     "interface": claude,
        #     "supports_web": True,
        #     "temporarily_down": False,
        #     "last_answer": "",
        #     "last_struct": "",
        #     "last_agree": False,
        # },
        # {
        #     "id": "qwen",
        #     "name": "Alibaba Qwen",
        #     "interface": qwen,
        #     "supports_web": False,  # ä½ çš„æ¥å£é‡Œä¹Ÿæ²¡æœ‰ web search
        #     "temporarily_down": False,
        #     "last_answer": "",
        #     "last_struct": "",
        #     "last_agree": False,
        # },
    ]

    loop_idx = 0

    with ThreadPoolExecutor(max_workers=len(models)) as executor:
        # ===== é¦–è½®ï¼šæ‰€æœ‰æ¨¡å‹å„è‡ªå›ç­”åŸé—®é¢˜ï¼ˆå¹¶è¡Œï¼‰ =====
        print("=== Initial Round: All models answer the original prompt ===")
        init_futs = {}
        for m in models:
            if m["supports_web"]:
                init_futs[m["id"]] = executor.submit(
                    m["interface"].ask, prompt, True
                )
            else:
                init_futs[m["id"]] = executor.submit(
                    m["interface"].ask, prompt
                )

        for m in models:
            mid = m["id"]
            try:
                raw = init_futs[mid].result()
                m["last_answer"] = raw
                log.append(
                    f"=== Initial {m['name']} Answer ===\n"
                    f"{raw}\n"
                    f"(timestamp: {datetime.now().isoformat()})"
                )
            except Exception as e:
                m["temporarily_down"] = True
                m["last_answer"] = f"[{m['name']} åœ¨é¦–è½®å›ç­”ä¸­å‘ç”Ÿé”™è¯¯: {repr(e)}]"
                log.append(
                    f"=== Initial {m['name']} Error ===\n"
                    f"Error: {repr(e)}\n"
                    f"è¯¥æ¨¡å‹åœ¨é¦–è½®æœªèƒ½æˆåŠŸè¿”å›ï¼Œå°†è¢«è§†ä¸ºæœ¬è½®æ‰çº¿ï¼Œä½†åœ¨åç»­è½®æ¬¡ä»ä¼šå°è¯•é‡æ–°åŠ å…¥ã€‚\n"
                    f"(timestamp: {datetime.now().isoformat()})"
                )
        
        print("=== Initial Round Complete. Starting Debate Loops ===")
        # ===== ä¸»å¾ªç¯ï¼šå¤šè½®è¾©è®º =====
        while loop_idx < max_loops:
            loop_idx += 1
            print(f"\n=== Loop {loop_idx} ===")

            # æœ¬è½®å‚ä¸è€…ï¼šä¸Šä¸€è½®æ²¡æœ‰ temporarily_down çš„æ¨¡å‹
            participants = [m for m in models if not m["temporarily_down"]]

            if len(participants) < 2:
                log.append(
                    f"=== Loop {loop_idx}: Not enough participants (n={len(participants)}), stop debating. ===\n"
                    f"(timestamp: {datetime.now().isoformat()})"
                )
                break

            prompts = {}
            for m in participants:
                prompts[m["id"]] = build_debate_prompt(m, participants)

            futs = {}
            for m in participants:
                mid = m["id"]
                if m["supports_web"]:
                    futs[mid] = executor.submit(
                        m["interface"].ask, [prompts[mid]], True
                    )
                else:
                    futs[mid] = executor.submit(
                        m["interface"].ask, [prompts[mid]]
                    )

            # é»˜è®¤ä¸‹ä¸€è½®å¤§å®¶éƒ½â€œæœ‰èµ„æ ¼â€å‚ä¸ï¼›è¿™ä¸€è½®æŠ¥é”™çš„å†æ ‡è®° temporarily_down=True
            for m in models:
                m["temporarily_down"] = False

            for m in participants:
                mid = m["id"]
                try:
                    raw = futs[mid].result()
                    m["last_struct"] = raw
                    agree, answer = parse_list_response(raw)
                    m["last_agree"] = agree
                    m["last_answer"] = answer

                    print(f"{m['name']} parsed:", agree, "(answer length:", len(answer), ")")
                    log.append(
                        f"=== Loop {loop_idx}: {m['name']} Evaluation ===\n"
                        f"Prompt to {m['name']}:\n{prompts[mid]}\n\n"
                        f"Raw output:\n{raw}\n\n"
                        f"Parsed -> agree: {agree}, answer length: {len(answer)}\n"
                        f"(timestamp: {datetime.now().isoformat()})"
                    )

                except Exception as e:
                    m["temporarily_down"] = True
                    m["last_struct"] = f"[{m['name']} åœ¨ç¬¬ {loop_idx} è½®ä¸­è°ƒç”¨å¤±è´¥: {repr(e)}]"
                    msg = f"[{m['name']} åœ¨ç¬¬ {loop_idx} è½®ä¸­è°ƒç”¨å¤±è´¥: {repr(e)}]"
                    print(msg)
                    log.append(
                        f"=== Loop {loop_idx}: {m['name']} Error ===\n"
                        f"Prompt to {m['name']}:\n{prompts[mid]}\n\n"
                        f"Error: {repr(e)}\n"
                        f"è¯¥æ¨¡å‹åœ¨æœ¬è½®è¢«è§†ä¸ºæ‰çº¿ï¼Œä½†ä¸‹ä¸€è½®ä»ä¼šå°è¯•é‡æ–°åŠ å…¥ã€‚\n"
                        f"(timestamp: {datetime.now().isoformat()})"
                    )

            successful_models = [
                m for m in models
                if not m["temporarily_down"] and m["last_struct"]
            ]
            if successful_models and all(m["last_agree"] for m in successful_models):
                print("\nâœ… æœ¬è½®æ‰€æœ‰æˆåŠŸè¿”å›çš„æ¨¡å‹ agree=Trueï¼Œè®¤ä¸ºå·²è¾¾æˆå…±è¯†ã€‚")
                log.append(
                    "=== Final Agreement (All successful models True) ===\n"
                    + "\n".join(
                        [
                            f"{m['name']} agree flag: {m['last_agree']}, answer length: {len(m['last_answer'])}"
                            for m in successful_models
                        ]
                    )
                    + f"\n(timestamp: {datetime.now().isoformat()})"
                )
                break

        # ===== ç»“æŸï¼šè®©æ‰€æœ‰â€œæœ€è¿‘æ›¾ç»æˆåŠŸè¿”å›è¿‡â€çš„æ¨¡å‹å†™æœ€ç»ˆé•¿ç¯‡ =====
        final_futs = {}
        for m in models:
            if not m["last_answer"] and not m["last_struct"]:
                continue
            final_prompt = [build_final_prompt(m)]
            if m["supports_web"]:
                final_futs[m["id"]] = executor.submit(
                    m["interface"].ask, final_prompt, True
                )
            else:
                final_futs[m["id"]] = executor.submit(
                    m["interface"].ask, final_prompt
                )

        final_answers_map = {}
        for m in models:
            mid = m["id"]
            if mid not in final_futs:
                continue
            try:
                ans = final_futs[mid].result()
                final_answers_map[mid] = ans
                log.append(
                    f"=== Final Long Answer from {m['name']} ===\n"
                    f"{ans}\n"
                    f"(timestamp: {datetime.now().isoformat()})"
                )
            except Exception as e:
                final_answers_map[mid] = f"[{m['name']} åœ¨æœ€ç»ˆé•¿ç¯‡å›ç­”é˜¶æ®µå‡ºé”™: {repr(e)}]"
                log.append(
                    f"=== Final Long Answer Error from {m['name']} ===\n"
                    f"Error: {repr(e)}\n"
                    f"(timestamp: {datetime.now().isoformat()})"
                )

    # ===== æ”¶å°¾ï¼šæ±‡æ€»ç»“æœ =====

    def pick_final(mid):
        m = get_model(models, mid)
        if m is None:
            return f"[{mid} æ¨¡å‹ä¸å­˜åœ¨]"
        if mid in final_answers_map:
            return final_answers_map[mid]
        if m["last_answer"]:
            return m["last_answer"]
        if m["last_struct"]:
            return m["last_struct"]
        return f"[{m['name']} æœªèƒ½ç»™å‡ºæœ‰æ•ˆç­”æ¡ˆ]"

    gemini_final = pick_final("gemini")
    gpt_final = pick_final("chatgpt")
    deepseek_final = pick_final("deepseek")
    # claude_final = pick_final("claude")
    # qwen_final = pick_final("qwen")

    log.append(
        "=== Final Summary ===\n"
        + "\n".join(
            [
                f"{m['name']} temporarily_down (last loop): {m['temporarily_down']}, "
                f"has_last_answer: {bool(m['last_answer'])}"
                for m in models
            ]
        )
        + f"\n(timestamp: {datetime.now().isoformat()})"
    )

    write_log_to_file(log, log_filename)
    write_final_answers_to_file(
        gemini_final,
        gpt_final,
        deepseek_final,
        # claude_final,
        # qwen_final,
        filename=final_answers_filename,
    )
    print(f"\nğŸ“ Dialog log exported to {log_filename}")
    print(f"ğŸ“ Final answers exported to {final_answers_filename}")

    last_gemini_struct = get_model(models, "gemini")["last_struct"]
    last_gpt_struct = get_model(models, "chatgpt")["last_struct"]
    last_deepseek_struct = get_model(models, "deepseek")["last_struct"]
    # last_claude_struct = get_model(models, "claude")["last_struct"]
    # last_qwen_struct = get_model(models, "qwen")["last_struct"]

    return (
        gemini_final,
        gpt_final,
        deepseek_final,
        # claude_final,
        # qwen_final,
        last_gemini_struct,
        last_gpt_struct,
        last_deepseek_struct,
        # last_claude_struct,
        # last_qwen_struct,
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run multi-model debate loop with optional prefix for file naming"
    )
    parser.add_argument(
        "--prefix",
        type=str,
        default=None,
        help="Prefix for input/output files (e.g., 'wzw' -> prompt_wzw.md, logs/wzw/final_answers_wzw.md, logs/wzw/dialog_log_wzw.md)"
    )
    args = parser.parse_args()
    
    # Load prompt based on prefix
    prompt = load_prompt(prefix=args.prefix)
    
    # Set output directory and filenames based on prefix
    if args.prefix:
        output_dir = f"logs/{args.prefix}"
        log_filename = f"{output_dir}/dialog_log_{args.prefix}.md"
        final_answers_filename = f"{output_dir}/final_answers_{args.prefix}.md"
    else:
        output_dir = "logs/default"
        log_filename = f"{output_dir}/dialog_log.md"
        final_answers_filename = f"{output_dir}/final_answers.md"
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    (
        final_gemini,
        final_gpt,
        final_deepseek,
        # final_claude,
        # final_qwen,
        gemini_debug,
        gpt_debug,
        deepseek_debug,
        # claude_debug,
        # qwen_debug,
    ) = close_loop_ask(
        prompt,
        max_loops=30,
        log_filename=log_filename,
        final_answers_filename=final_answers_filename,
        prefix=args.prefix,
    )
    print("\n=== æœ€ç»ˆä¸­æ–‡ç­”æ¡ˆï¼ˆé‡ç­”åŸå§‹ä»»åŠ¡ï¼‰ ===")
    print("Gemini:", final_gemini)
    print("ChatGPT:", final_gpt)
    print("DeepSeek:", final_deepseek)
    # print("Claude:", final_claude)
    # print("Qwen:", final_qwen)
